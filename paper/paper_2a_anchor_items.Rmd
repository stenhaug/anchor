Psychometricians have long been in search of the perfect DIF detection method. In this section, we summarize existing DIF methods and propose extensions of those methods. We take as given that the IRT-based likelihood ratio test is the best way to test for DIF, which has been well demonstrated [@meade2012solving]. As a result, we avoid methods like the Mantel-Haenszel procedure [@holland1986differential], which muddies the waters by moving away from the IRT framework, and has been shown to perform no better — and in some cases worse — than IRT-based methods [@swaminathan1990detecting]. 

The unsolved and interesting problem is how to link groups in the common circumstance in which no a priori assumptions — about either relative group ability or which items contain bias — can be made. For simplicity, we focus on the Rasch model to isolate the fundamental issues in DIF detection. And, in search of a coherent framework, we sometimes edit names of existing methods. We recognize that others have done the same [e.g. @kopf2015framework] and that we risk contributing to the proliferation of names. 

We use a simple, one-run simulation to demonstrate the methods: 10,000 reference group students and 10,000 focal group students taking an eight-item test. Target ability is simulated ${\theta_i}^{\text{ref}} \sim N(0,1)$ for students in the reference group and ${\theta_i}^{\text{foc}} \sim N(-1,1)$ for students in the focal group. Nuisance ability is set to ${\eta_i}^{\text{ref}} = 0$ and ${\eta_i}^{\text{ref}} = -1$. The slope on target ability is set to $a_{1j} = 1$ for all items, and the slope on nuisance ability is set to $a_{2j} = 0.5$ for the last three items (the items with DIF) and $a_{2j} = 0$ otherwise. From the DIF-as-item-property view, there is no nuisance ability. Instead, ${b_j}^{\text{foc}} = {b_j}^{\text{ref}}$ for the first five items and ${b_j}^{\text{foc}} = {b_j}^{\text{ref}} - 0.5$ for the last three items.

In general, we denote the mean ability of the reference and focal group as $\mu^\text{ref}$ and $\mu^\text{foc}$, respectively. We follow the common and inconsequential practice of identifying the scale by setting $\mu^\text{ref} = 0$. The fundamental challenge for DIF methods is to disentangle estimating $\hat\mu^\text{foc}$ from estimating $\hat{b_j}^{\text{foc}} - \hat{b_j}^{\text{ref}}$. The most common approach is to identify a group of anchor items that are assumed to be DIF-free. These anchor items identify the model, thereby allowing for the estimation of $\hat\mu^\text{foc}$.

## Anchor items

### All-others-as-anchors

@meade2012solving compared the most commonly used methods and unequivocally recommended the all-others-as-anchors (AOAA). AOAA tests each item for DIF one at a time using all of the other items as anchors. For example, when testing the first item for DIF, all of the other items are used as anchors. This is done using a likelihood ratio test that compares the baseline model, where all item parameters are fixed across groups, to the flexible model, where the parameters of the tested item are freed across groups [@thissen1993detection]. Then, when testing the second item for DIF, once again all of the other items (including the first item) are used as anchors, and so on. The items for which the flexible model outperforms the baseline model (typically based on a chi-squared test) are identified as having DIF and the rest of the items become anchor items.

@edelen2006identification used AOAA to look for DIF between the English and Spanish versions of the 21-item Mini-Mental State Examination and found that 10 of the 21 items exhibited DIF. How can we be sure its those 10 items and not the other 11 items with DIF? We can't be. Implicit in the use of AOAA is the assumption that all items not being tested do not exhibit DIF, which is, of course, impossible. More practically, it is thought that AOAA will perform well if a small minority of items have DIF or the DIF is balanced such that some items are biased against the focal group and others are biased against the reference group. Undesirably, most applications of DIF methods and many simulation studies do not make explicit the assumptions of the DIF method [@strobl2018anchor]. In this way, psychometricians might benefit from adopting economists' habit of explicitly stating assumptions and debating their plausibility. 

Researchers have noticed the circular logic of AOAA, but have mostly described it indirectly by pointing out inflated Type I errors in simulation studies [@stark2006detecting]. A simple thought experiment illustrates how AOAA fails: Imagine a test with a sufficiently large number of students and three items where the first item has DIF, and the other two do not. Using AOAA, all items test positive for DIF. The last two items incorrectly test positive because including the first item in the anchor set causes the group ability difference to be misestimated. This phenomenon of items with real DIF inducing the appearance of DIF in other items was only indirectly discussed in the literature until @andrich2012real coined the term "artificial DIF."

#### All-others-as-anchors-all-significant

One way to attempt to counter artificial DIF is with purification through iterative anchor selection. For example, @drasgow1987study started with AOAA, removed items displaying DIF from the anchor set, then repeated the process iteratively — with items that have been removed from the anchor set allowed to have free parameters across groups in both the baseline and flexible model — until no more items tested positively. @kopf2015framework named this technique Iterative-backward-AOAA with "backward" (as in reverse, not incorrect) referring to beginning with the assumption that all items are DIF-free. We find it clearer to refer to this method as all-others-as-anchors-all-significiant (AOAA-AS). Appending all-significant indicates that anchor selection is done iteratively with all items that show statistical significance for DIF being removed from the anchor set. 

AOAA-AS might seem like an improvement but it doesn't solve a fundamental problem of AOAA: What do we do when all items test positive for DIF? With a sufficient sample size and at least one item with DIF, this will necessarily be the case. In our thought experiment, we get the same result with AOAA-AS as we did with AOAA: All items test positive for DIF and there are no anchor items. @kopf2015framework encountered precisely this problem in their simulation study and chose to select a single anchor item randomly. @woods2009empirical suggested a more straightforward, one-step method which uses AOAA and selects the, say, four items that exhibit the least amount of DIF. It's unclear if one should proceed if those four items display DIF themselves.

#### All-other-as-anchors-one-at-a-time

We propose an extension of these methods, all-others-as-anchors-one-at-a-time (AOAA-OAT), which, to our knowledge (and surprise), has not previously been explicitly proposed. AOAA-OAT is inspired by @hagquist2017recent who, in general, assert that "items showing DIF initially
should not be resolved simultaneously but sequentially" [p. 6]. Like AOAA-AS, AOAA-OAT starts with AOAA, but only the single item exhibiting the most DIF, based on the $\chi^2$ test statistic, is removed from the anchor set. The process continues iteratively until no new items display DIF. AOAA-OAT and AOAA-AS are similar in that they are both iterative; the difference is that AOAA-OAT takes the more conservative approach of removing only one item in each iteration as opposed to all items that test positive for DIF. As a result, AOAA-OAT is less likely to arrive at the wrong conclusion because of artifical DIF.

Applying AOAA-OAT to our thought experiment demonstrates its effectiveness. The initial AOAA removes the real DIF item from the anchor set because it exhibits the most DIF. In the next step, both of the other items test negative for DIF, and we arrive at the correct conclusion. AOAA-OAT has two assumptions: First, that at least one item does not have DIF, and second, that the majority of items do not have DIF. 

#### Summary and performance 

Table \ref{table:themethods} summarizes the three all-others-as-anchors methods. It's useful to remember that AOAA is not an iterative procedure, while the methods with a hyphen, AOAA-OAT and AAOA-AS, are iterative procedures with AOAA-OAT being our new, more conservative method.

<!-- the two kopf 2015 papers love the iterative forward SA. I bet AOAA-OAT is as good or better while fitting fewer models -->

\begin{table}[H]
\caption{EDIT THIS}
\centering
\begin{tabular}{|p{2.5cm}|p{6cm}|p{4cm}|}
\toprule

Method & Description & Literature \\

\midrule

AOAA & Test if each item has DIF by using all of the other items as anchors (not iterative). & Originally proposed by \cite{lord1980} and formalized by \cite{thissen1993detection} \\\hline

AOAA-AS & The first iteration is AOAA. All items that test positive for DIF are removed from the anchor set. Continue iterating until no new items test positive for DIF. & Proposed by \cite{drasgow1987study} \\\hline

AOAA-OAT & The first iteration is AOAA. Only the item that shows the most extreme DIF is removed from the anchor set. Continue iterating until no new items test positive for DIF. & To our knowledge, not proposed or used previously \\

\bottomrule
\end{tabular}
\label{table:themethods}
\end{table}

[ADD IN SIMULATION RESULTS]

### Clustering items

```{r}
# source(here("paper/paper_2_sim.R"))
# out_3_biased_items %>% write_rds(here("paper", "paper_2_out_3_biased_items.rds"))
out_3_biased_items <- read_rds(here("paper", "paper_2_out_3_biased_items.rds"))
```

@bechger2015statistical proposed choosing anchor items by identifying clusters of items that function similarly and then choosing one of those clusters to be the "anchor cluster." They pointed out that one way around the unidentifiability issue is to consider relative item parameters. For each group, the relative easinesses for each pair of items can be stored in the matrix $\mathbf{R}^{\text{ref}}$ with entries ${R_{xy}}^{\text{ref}} = {b_x}^{\text{ref}} - {b_y}^{\text{ref}}$. The ultimate matrix of interest is $\Delta \mathbf{R} \equiv \mathbf{R}^{\text{ref}}-\mathbf{R}^{\text{foc}}$ which is the "differences between groups in the pairwise differences in (easiness) between items" [p. 323].

The general idea of identifying clusters of items is intriguing. However, their approach is needlessly complicated and they did not describe a process for moving from $\Delta \mathbf{R}$ to a set of anchor items. @pohl2017cluster extended their work by proposing one such process. $\Delta \mathbf{R}$ is skew-symmetric and of rank 1, which means that all information is contained in a single row or column. Accordingly, they use k-means clustering on just the first column of $\Delta \mathbf{R}$ where the number of clusters, k, is chosen by minimizing BIC. They suggest using a combination of cluster size, cluster homogeneity, and cluster parameter precision to choose which of the clusters is the anchor cluster. Unfortunately, in their simulation study, they find that BIC identifies only a single cluster, so they end up using all of the items as anchors.

We propose a new cluster-based approach, which we call "equal means clustering" (EMC). Instead of working with an arbitrary column from $\Delta \mathbf{R}$, we work with the vector $\tilde{\mathbf{d}}$ of differences in item easiness, $\tilde{d_j} = \tilde{b_j}^\text{ref} - \tilde{b_j}^\text{foc}$, where the model is identified by setting $\mu^\text{foc} = 0$ (recall $\mu^\text{ref}$ is always set to $0$, thus the name "equal means clustering"). This results in all differences in performance — either from group ability differences or DIF — manifesting in the item easiness difference parameter $\tilde{d_j}$. The tilde is used to denote parameters estimated with $\mu^\text{foc}$ arbitrarily set to $0$.

Instead of choosing k with BIC, we use the gap statistic method recommended by @hastie2001estimating. The largest cluster is chosen as the anchor cluster. If there is a tie for the largest cluster, the cluster with the lowest standard deviation of $\tilde{\mathbf{d}}$ is selected. This process makes the assumption that the largest cluster (but necessarily the majority) of items do not contain DIF.

OUR SIMULATION RESULTS

```{r}
out_3_biased_items$cluster_status[[1]] %>% 
    select(item, a_ref_easy, b_foc_easy, difference_in_easy, cluster) %>% 
    mutate(anchor = ifelse(cluster == 1, TRUE, FALSE)) %>% 
    kable()
```

### Analyst chooses

DIF methods are generally designed to automatically detect DIF without any human judgement. On the other hand, it can be useful to present information to the analyst in a way that empowers them to see DIF and potentially even select the anchor items. 

We propose a technique, the "equal means, multiple imputation logit graph" (EM-MILG), which presents information about potential DIF to the analyst. Like EMC, EM-MILG begins by fitting a unidimensional Rasch model to the data that is identified by setting $\mu^\text{foc}$ to $0$. Again, the result is that all differences in performance manifest in $\tilde{\mathbf{d}}$. To get a sense of variation in each $\tilde{{d_j}}$, the item parameter covariance matrix is estimated using Oakes' identity [@chalmers2018numerical]. Then, multiple imputations (MI) [@yang2012characterizing] are used to estimate the distribution of $\tilde{d_j}$ for every item. These are the distributions displayed in the EM-MILG. The method is inspired in part by @pohl2017cluster who fit a model with both the reference and focal group means set to 0 in a pedagogical example, and @talbot2013taking who fixed both pre-test and post-test means to 0 in order to estimate item-specific learning gains. 

Figure \ref{fig:emmilg} shows the EM-MILG for our simulated data. It cannot be stated strongly enough that the EM-MILG contains all possible information about the difference in group performance. The challenge, then, is to select the anchor items. The analyst might assume that — because there are five items where the reference group outperforms the focal group by 1 logit and only three items where the difference is 1.5 logits — items 1-5 are unbiased and can be used as anchor items.

```{r emmilg, fig.cap = 'The equal means multiple imputations logit graph (EM-MILG) shows the distribution of how many logits the reference group outperforms the focal group by on each item.', out.width="70%", warning = FALSE, message = FALSE}
out_3_biased_items$intuitive_mod[[1]] %>%
    mod_intuitive_to_draws_df() %>%
    draws_df_to_logit_plot() +
    labs(
        x = latex2exp::TeX("$\\tilde{d_j} = \\tilde{b_j}^{ref} - \\tilde{b_j}^{foc}$")
    )

# labeling good enough for now maybe check here to fiddle more:
# https://stackoverflow.com/questions/8190087/italic-greek-letters-latex-style-math-in-plot-titles
```

After choosing the anchor items, the model is refit. The new model is identified by setting $d_j = 0$ for the anchor items, instead of by setting $\mu^\text{foc}$ to $0$. The same process of using multiple imputations to estimate the distribution of $d_j$ can be used with the new model. Because the equal means assumption is not made, we refer to the resulting graph as a multiple imputations logit graph (MILG). In our simulation, selecting the first five items as anchors correctly results in $\hat d_j \approx 0.5$ for the items with DIF as is shown in the MILG in Figure \ref{fig:milg}.

```{r milg, fig.cap = 'The multiple imputations logit graph (MILG) shows the distribution of DIF against the focal group. Anchor items are fixed to 0.', out.width="70%", warning = FALSE, message = FALSE}
# CAREFUL - this code assumes that the analyst would choose the same anchor items as AOAA-OAT found 

out_3_biased_items$AOAA_OAT_final_dif[[1]] %>% 
    mutate(difference_in_easy = difference_in_easy - difference_in_easy[anchor][1]) %>% 
    ggplot(aes(x = difference_in_easy, y = item)) +
    ggridges::geom_density_ridges(
        data = 
            out_3_biased_items$AOAA_OAT_mod[[1]] %>% 
            mod_to_draws_df(n = 1000) %>% 
            select(-run) %>%
            gather(var, val) %>% 
            filter(val != 0),
        aes(x = val, y = var)
    ) +
    geom_point() +
    labs(
        x = latex2exp::TeX("$d_j = b_j^{ref} - b_j^{foc}$"),
        y = ""
    )
```

One of our key concerns with typical DIF methods is that they can lull the analyst into a false sense of security. Too often, the analyst chooses a method, implements it, and then proceeds as if the method certainly identified the correct anchor items. The EM-MILG combats this concern by presenting all information clearly to the analyst. In the previous example, the analyst may be weary of their results having seen how arbitrary it was to conclude that the first five and not the last three items are unbiased. Even when other DIF methods are used, the analyst can use the EM-MILG as a first step in order to give them a sense of their item response data. And, of course, the MILG can be used to visualize DIF anytime a model is fit using anchor items, not just when the first step involves the EM-MILG.
