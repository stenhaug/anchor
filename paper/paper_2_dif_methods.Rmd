Psychometricians have long been in search of the perfect DIF detection method. @meade2012solving compared the most commonly used methods and unequivocally recommended the all-others-as-anchors (AOAA) method, where each item is tested one at a time for DIF using all other items as anchors. More specifically, testing is done using a nested model comparison between the baseline model, where all item parameters are fixed across groups, and the flexible model, where the parameters of the tested item are freed across groups. Anchor items have fixed parameters across groups and consequently are used to estimate the ability difference between groups. For example, @edelen2006identification used AOAA to look for DIF between the English and Spanish versions of the 21-item Mini-Mental State Examination and found that 10 of the 21 items exhibited DIF. How can we be sure its those 10 items and not the other 11 items with DIF? Implicit in the use of AOAA is the assumption that all items not being tested do not exhibit DIF, which is, of course, impossible. Undesirably, most applications of DIF methods and many simulation studies do not make explicit the assumptions of the DIF method @strobl2018anchor. In this way, psychometricians might benefit from adopting economists' habit of explicitly stating assumptions and debating their plausibility. 

Researchers have noticed the circular logic of AOAA, but have mostly described it indirectly by pointing out inflated Type I errors in simulation studies [@stark2006detecting]. A simple thought experiment illustrates how AOAA fails: Imagine a test with a sufficiently large number of students and three items where the first item has DIF, and the other two do not. Using AOAA, all items test positive for DIF. The last two items incorrectly test positive because including the first item in the anchor set causes the group ability difference to be misestimated. This phenomenon of items with real DIF inducing the appearance of DIF in other items was only indirectly discussed in the literature until @andrich2012real coined the term "artificial DIF."

One way to attempt to counter artificial DIF is with purification with iterative anchor selection. For example, [@drasgow1987study] started with AOAA, removed items displaying DIF from the anchor set, then continued iteratively to test the remaining items for DIF until no more items tested positively. [@kopf2015framework] named this technique Iterative-backward-AOAA with "backward" (as in reverse, not incorrect) referring to beginning with the assumption that all items are DIF free. We find this name confusing and instead refer to this method as all-others-as-anchors-all-significiant (AOAA-AS) where appending all-significant indicates that anchor selection is done iteratively with all items that show statistical significance for DIF are removed from the anchor set. AOAA-AS might seem like an improvement, but what does one do when all items display DIF in the first AOAA stage? With a sufficient sample size, this will necessarily be the case. @kopf2015framework encountered precisely this problem in their simulation study and chose to select a single anchor item randomly. Woods suggested a more straightforward, one-step method which uses AOAA and selects the, say, four items that exhibit the least amount of DIF. It's unclear if one should proceed if those four items display DIF themselves. 

We propose an extension of these methods, all-others-as-anchors-one-at-a-time (AOAA-OAAT), which, to our knowledge (and surprise), has not previously been proposed. AOAA-OAT starts with AOAA, and the single item exhibiting the most DIF is removed from the anchor set. Each item in the anchor set is then tested for DIF again (with items outside of the anchor set allowed to have free parameters across groups in both the baseline and flexible model). This process continues until no new items display DIF. AOAA-OAAT and AOAA-AS are similar in that they are both iterative; the difference is that AOAA-OAAT takes the more conservative approach of removing only the item exhibiting the most DIF. Applying AOAA-OAT to our thought experiment demonstrates its logic. The initial AOAA removes the real DIF item from the anchor set because it exhibits the most DIF. In the next step, both of the other items test negative for DIF, and we arrive at the correct conclusion. AOAA-OAT has two assumptions: First, that at least one item does not have DIF, and second, that the majority of items do not have DIF. Table X summarizes. 
<!-- the two kopf 2015 papers love the iterative forward SA. I bet AOAA-OAT is as good or better while fitting fewer models -->

\begin{center}
 \begin{tabular}{||c c c c c||} 
 \hline
 Name & Abbreviation & Description & Assumptions & Literature \\ [0.5ex] 
 \hline\hline
 All-others-as-anchors & AOAA & 87837 & 787 & 1\\ 
 \hline
 All-others-as-anchors-all-significant & AOAA-AS & 78 & 5415 & 1\\
 \hline
 All-others-as-anchors-one-at-a-time & AOAA-OAAT & 778 & 7507 & 1\\
 \hline
 4 & 545 & 18744 & 7560 & 1\\
 \hline
 5 & 88 & 788 & 6344 & 1\\ [1ex] 
 \hline
 \end{tabular}
\end{center}

DIF methods, in general, seem to be designed to automatically detect items with DIF without any intervention from a human. In some cases, it might make sense to present information to the analyst in a way that empowers them to make decisions. Our preferred technique, which we refer to as the "equal means logit graph" (EMLG), is to set both groups means to 0 so that differences in performance to manifest in the item parameters. Relatedly, @pohl2017cluster fit a model with both groups means set to 0 in a pedagogical example, and @talbot2013taking fixed both pre-test and post-test means to 0 in order to estimate item-specific learning gains. 

We use a simple simulation to demonstrate EMLG. 10,000 reference group students and 10,000 focal group students take an eight-item test. Target ability is simulated ${\theta_i}^{\text{ref}} \sim N(0,1)$ for students in the reference group and ${\theta_i}^{\text{foc}} \sim N(-1,1)$ for students in the focal group. Nuisance ability is set to ${\eta_i}^{\text{ref}} = 0$ for the reference group and ${\eta_i}^{\text{ref}} = -1$ for the focal group. The slope on target ability is set to $a_{1j} = 1$ for all items, and the slope on nuisance ability is set to $a_{2j} = 0.5$ for the last three items (the items with DIF) and $a_{2j} = 0$ otherwise. We fit a unidimensional Rasch, with both group means fixed to 0, to the simulated item response data. We estimate the item parameter covariance matrix using Oakes' identity [@chalmers2018numerical]. Multiple imputations (MI) [@yang2012characterizing] are used to estimate the distribution of the difference in difficulty, ${b_j}^{\text{foc}} - {b_j}^{\text{ref}}$, for each item. Figure \ref{fig:emlg} shows these distributions.

```{r emlg, fig.cap = 'Equal means logit graph (EMLG)', out.width="70%", warning = FALSE, message = FALSE}
source("paper_2_sim.R")

out_3_biased_items$intuitive_mod[[1]] %>% 
    mod_intuitive_to_draws_df() %>% 
    mutate_at(vars(starts_with("item")), function(x) -x) %>% 
    draws_df_to_logit_plot() +
    labs(
        x = latex2exp::TeX("$b_j^{foc} - b_j^{ref}$ (group means fixed to 0)")
    )

# labeling good enough for now maybe check here to fiddle more:
# https://stackoverflow.com/questions/8190087/italic-greek-letters-latex-style-math-in-plot-titles
```

It cannot be stated strongly enough that Figure \ref{fig:emlg} contains all possible information about the difference in group performance. The challenge, then, is to identify the difference in group ability. The analyst might assume that, because there are five items showing a difference of about 1 logit and only three items showing a difference of 1.5 logits, the group ability difference is 1 logit and items 6, 7, and 8 contain approximately 0.5 logit bias against the focal group. The implicit assumption they are making is that it is the minority of items that contain DIF.

DIF methods use an algorith with its own set of assumptions to identify the group ability difference. 

The beauty of the EMLG graph is it makes clear all information instead of allowing the analyst the false sense of security of running an algorithm and wholeheartedlyr believing the output. 

We know that this test contains DIF. Which items depends on ability difference which we can be sure of. We can, however, quantify the amount of DIF on this test. We can take the SD or we can assume between and calculate chalmers statistic for each. We might not know for sure the ability difference but we can quantify the potential for DIF. For each we calculate the SD across item parameters. 



