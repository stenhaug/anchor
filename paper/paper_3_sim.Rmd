To compare each of the methods in table TODO, we conducted a simulated study. Our goal was to make the data generating process as realistic to the scenario described by @ackerman1992didactic in which some items on a math test also depend on a student’s verbal ability (the target ability is math ability and the nuisance ability is verbal ability). As described in [the introduction](#intro), nearly all DIF simulation studies simply generate data where the item easiness parameters are different for the reference and focal groups. This setup can be re-written as a two-dimensional compensatory item response model where nuisance ability is the same for all students from the same group. One exception is @walker2017using who draws each student's target and nuisance ability from a two-dimensional normal distribution with varying covariance matrices. 

In our simulation study, it was critical that student ability was drawn in a realistic way similar to @walker2017using. However, we don’t believe that a compensatory model is realistic in describing the a math test where some items depend on verbal ability. For example, it’s hard to imagine that a student without the verbal ability to parse a word problem could fully compensate by having a higher math ability. Accordingly, we generate item responses using an adapted version of Sympson’s -@sympson1978model noncompensatory item response model in which 
$$
\text{Pr}(y_{ij} = 1 | \theta_i, \eta_i, a_{2j}) = \sigma(\theta_i) \cdot \sigma(a_{2j}\eta_i)
$$
where, as before, $\theta_i$ is target ability, $\eta_i$ is nuisance ability, and $\sigma(a_{2j})$ is the item's loading on nuisance ability [@demars2016partially].

## Drawing parameters

In each run, we simulate 10,000 students with half coming from each of the reference and focal groups. For students from the reference group, target ability and nuisance ability are drawn from the two-dimensional normal distribution with mean [$\mu_\theta^\text{ref} = 0$, $\mu_\eta^\text{ref} = 0$] and covariance matrix $\begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$. Abilities for students from the focal group are drawn using the same covariance matrix but with means [$\mu_\theta^\text{foc} = -0.5$, $\mu_\eta^\text{foc} = -1$].

The test always has 20 items, but we vary the number of items with DIF from two to ten. For items without DIF, $a_{2j} = \infty$ so that the model reduces to $\text{Pr}(y_{ij} = 1 | \theta_i) = \sigma(\theta_i)$. For items with DIF, $a_{2j}$ is calculated based on Ackerman's -@ackerman1994using angle equation as described in @walker2017using:
$$
\angle_j = \arccos \dfrac{a_{1j}^2}{a_{1j}^2 + a_{2j}^2}.
$$
The angle for an item measure the relative loading of the item on the two dimensions. For example, an angle of 45$^\circ$ indicates that the item loads equally on the target and nuisance ability dimensions. We are using a simple compensatory model in which $a_{1j} = 1$ for all items so the angle equation reduces to 
$$
\angle_j = \arccos \dfrac{1}{1 + a_{2j}^2}.
$$
We are interested in specifying the angle of an item, so the relevant equation becomes 
$$
a_{2j} = \sqrt{\dfrac{1 - \cos(\angle_j)^2}{\cos(\angle_j)^2}}.
$$
For DIF items, we set $a_{2j}$ based on angles with equal intervals between 30$^\circ$ and 60$^\circ$. For example, for a test with four DIF items the angles are 30$^\circ$, 40$^\circ$, 50$^\circ$, and 60$^\circ$.

## Visualizing a run
```{r}
# ADD IN CODE THAT GENERATES THIS
out <- read_rds(here("paper/paper_3_out1.rds"))

sim_n_dif_items_to_foc_mean <- function(sim, n_dif_items){
    mod <-
        mod_flexible(
            sim$data,
            sim$groups,
            flex_items = (ncol(sim$data) - n_dif_items + 1):ncol(sim$data)
        )

    coef_1f(mod)$ability$b_foc[1]
}

add_foc_mean <- function(one, n_dif_items){
    one %>%
    mutate(foc_mean = sim %>% map_dbl(sim_n_dif_items_to_foc_mean, n_dif_items))
}

out <-
    out %>%
    mutate(bigsim = map2(bigsim, n_dif_items, add_foc_mean))

# i have no idea why this is like this...
out %>% 
  mutate(bigsim = bigsim %>% map(~ select(., foc_mean))) %>% 
  unnest(bigsim) %>% 
  ggplot(aes(x = n_dif_items, y = foc_mean)) +
  geom_point()

```
Figure \ref{fig:difmap} provides intuition about the data generating process by showing the relationship between $\theta_i$ and $Pr(y_{ij} = 1)$ with $\eta_i$ set to the group mean. The items are ordered by the amount of DIF such that $\angle_{j = 11} = 30^\circ$ up to $\angle_{j = 20} = 60^\circ$.
```{r difmap, fig.cap = 'For a 20 item test, the relationship between target ability and probability of correct response with nuisance abilities fixed to group means.', out.width="70%", warning = FALSE, message = FALSE}
out$bigsim[[2]]$pars[[4]] %>% 
  graph_pars_noncompensatory(0, -1) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = latex2exp::TeX("$\\theta_i$"),
    y = latex2exp::TeX("$Pr(y_{ij} = 1)$")
  )
```

Figure \ref{fig:simemmlg} shows the EM-MILG for one run using the same item parameters that generated Figure \ref{fig:difmap}. As expected, $\tilde{d_j}$ is about $\mu_\theta^\text{foc} - \mu_\theta^\text{ref} = -0.5 - 0 = -0.5$ for the first ten items which are DIF free. For the last ten items, $\tilde{d_j}$ increases as $\angle_j$ increases. Of course, we cannot include the "analyst chooses" method in our simulation, but regardless, the EM-MILG is a useful way to visualize the data.
```{r simemmlg, fig.cap = 'For a 20 item test, the relationship between target ability and probability of correct response with nuisance abilities fixed to group means.', out.width="70%", warning = FALSE, message = FALSE}
out$bigsim[[2]]$intuitive_mod[[4]] %>%
    mod_intuitive_to_draws_df() %>%
    draws_df_to_logit_plot() +
    labs(
          x = latex2exp::TeX("$\\tilde{d_j} = \\tilde{b_j}^{ref} - \\tilde{b_j}^{foc}$")
      )
```

## Outcomes

### Target ability achievement gap
An effective method, should find $\hat\mu_\theta^\text{foc}$ close to 0.5. This outcome measures a method’s ability to disentangle differences in target ability from nuisance ability at the group level.

### Individual abilities
Assessments are frequently used to make decisions about individual student abilities. We measure a method’s ability to do so by calculating the rank correlation between the vector estimated abilities and true abilities. This is done both for all students combined and just for students in the focal group. We use rank correlation as opposed to the more typically used RMSE as a result of our agreement with Lord's -@lord1986maximum argument that RMSE can be inflated by poor performance at the extremes of ability for which the test was not designed to measure precisely.

### Anchor items. 
For the methods that choose a set of anchor items, we look directly at which anchor items are selected. An effective method should use most of the non-DIF items as anchors (the anchor hit rate) while avoiding including items with DIF in the anchor set (the false anchor rate).

## Results
In total, we executed 100 runs for each number of two to ten DIF items. This multiplies to a total of 900 runs. Figure \ref{fig:achievegap} shows each method's performance on estimating the target ability achievement gap. 

As expected, artificial DIF causes AOAA and AOAA-AS to go off the rails as the number of DIF items increases. CLUSTERING typically performs well but has a higher variance because the anchor cluster sometimes contains as few as half of the DIF-free items. 

AOAA-OAT, GINI, and MINBC all perform similarly well. MINBC has a tendency to slightly overestimate the achievement gap. There is an interesting pattern, not previously described in the literature, where AOAA-OAT and GINI of overestimate the achievement gap when there are few items with DIF and underestimate the achivement gap when there are many items with DIF. The cause is more easily understood with AOAA-OAT where we can inspect which items are included in the anchor set.

Figure \ref{fig:anchorfalse} shows the mean false anchor counts for each method and number of items with DIF. For example, when there are 10 items with DIF on the test, AOAA-AS incorrectly includes an average of 1.8 of those items in the anchor set. AOAA-OAT remarkably never includes an item with DIF in the anchor set (i.e., a false anchor rate of 0\%). 

Performance, then, is driven by which items are included in the anchor set. Accordingly, Figure \ref{fig:anchorhit} shows the number of items without DIF that are included in the anchor set. In general, AOAA-OAT usually fails to include a couple of items in the anchor set that are indeed DIF free. When there are a few items with DIF, the omitted items are more often items in which the focal group happened to, as a result of sampling variability, perform well relative to the reference group. On the other hand, when there are many items with DIF, the omitted items are more often items in which the focal group happened to perform poorly relative to the reference group. This selective omission of DIF free anchors is what drives the positive association between number of DIF items and $\hat\mu_\theta^\text{foc}$.

```{r achievegap, fig.cap = 'Each method's estimation of the target ability achievement gap.', out.width="70%", warning = FALSE, message = FALSE}
final_model_to_anchor_summary <- function(final_model, n_dif_items){
    coef_1f(final_model)$items %>%
        mutate(
            true_anchor =
                c(
                    rep("no dif", n_items - n_dif_items),
                    rep("some dif", ceiling(n_dif_items / 2)),
                    rep("high dif", floor(n_dif_items / 2))
                )
        ) %>%
        group_by(true_anchor) %>%
        summarize(selected = sum(anchor), total = n())
}

getfull <- function(one){
    one %>%
        select(ends_with("mod"), -intuitive_mod) %>%
        mutate(run = row_number()) %>%
        gather(method, model, -run) %>%
        mutate(
            n_anchors = model %>% map_int(~ sum(coef_1f(.)$items$anchor)),
            foc_mean = model %>% map_dbl(~ coef_1f(.)$ability$b_foc[1])
        ) %>%
        filter(n_anchors != 0 | method %in% c("gini_mod", "minbc_mod"))
}

out %>%
    mutate(achievegap = bigsim %>% map(getfull)) %>%
    select(-bigsim) %>%
    unnest(achievegap) %>%
    ggplot(aes(x = n_dif_items, y = foc_mean)) +
    geom_point(alpha = 0.5) +
    facet_wrap(~ method) +
    geom_hline(yintercept = -0.5, linetype = "dashed") +
    labs(
      x = "Number of DIF items",
      y = latex2exp::TeX("$\\hat{\\mu}_\\theta^foc$")
    )
```

```{r anchorfalse, fig.cap = 'Mean false anchor counts', out.width="70%", warning = FALSE, message = FALSE}
one_to_anchors <- function(one, n_dif_items){
    one %>%
        select(ends_with("mod"), -intuitive_mod, -gini_mod, -minbc_mod) %>%
        mutate(run = row_number()) %>%
        gather(method, model, -run) %>%
        mutate(anchor_summary = model %>% map(final_model_to_anchor_summary, n_dif_items)) %>%
        select(-run, -model) %>%
        unnest(anchor_summary) %>%
        group_by(method, true_anchor, total) %>%
        summarize(mean_selected = mean(selected)) %>%
        ungroup()
}

temp <-
    out %>%
    mutate(anchors = map2(bigsim, n_dif_items, one_to_anchors)) %>%
    select(-bigsim) %>%
    unnest(anchors)

temp %>%
    filter(true_anchor != "no dif") %>%
    group_by(n_dif_items, method) %>%
    summarize(total = sum(total), mean_selected = sum(mean_selected)) %>%
    gather(var, val, -n_dif_items, -method) %>%
    mutate(var = factor(var, 
                        levels = c("total", "mean_selected"),
                        labels = c("Total possible", "Mean selected to anchor set"))) %>% 
  filter(var == "Mean selected to anchor set") %>% 
    ggplot(aes(x = n_dif_items, y = val)) +
    geom_point() +
    ylim(0, 20) +
    facet_wrap(~ method) +
    labs(
      x = "Number of items with DIF on the test",
      y = "Mean number of items with DIF in the anchor set",
      color = ""
    )
```

```{r anchorhit, fig.cap = 'Mean anchor hit counts', out.width="70%", warning = FALSE, message = FALSE}
temp %>%
    filter(true_anchor == "no dif") %>%
    gather(var, val, -n_dif_items, -method, -true_anchor) %>%
    mutate(var = factor(var, 
                        levels = c("total", "mean_selected"),
                        labels = c("Total possible", "Mean selected to anchor set"))) %>% 
    ggplot(aes(x = n_dif_items, y = val, color = var)) +
    geom_point() +
    ylim(0, 20) +
    facet_wrap(~ method) +
    labs(
      x = "Number of items with DIF on the test",
      y = "Number of items without DIF",
      color = ""
    )
```
