Following @camilli1992conceptual, we conceptualize differential item functioning (DIF) as a varying relationship between ability and probability of correct response for students from different groups that manifests when one imposes an item response model with too few ability dimensions. From this perspective, the term "differential item functioning" is, perhaps, a misnomer as DIF is better thought of as a property of the student, as opposed to the item. For example, @ackerman1992didactic describes a scenario in which a test intends to measure a student's math ability, but performance also depends on their verbal ability. In this case, math ability is the "target ability", and verbal ability is the "nuisance ability". Fitting a unidimensional item response model to this test results in students with low verbal ability receiving a score systematically lower than their true math ability; therein lies "DIF".

Despite this, the usual setup of DIF simulation studies frames DIF as a property of the item. For example, @kopf2015framework simulate students as belonging to either a reference or focal group. They fix the item easinesses for the reference group, ${b_j}^{\text{ref}}$, to values that they obtained from a previous study [@wang2012dif]. They set item easinesses for the focal group to ${b_j}^{\text{foc}} = {b_j}^{\text{ref}}$ for items without DIF and ${b_j}^{\text{foc}} = {b_j}^{\text{ref}} - 0.6$ for items with DIF, where 0.6 is the magnitude of DIF in logits. They then simulate student ability ${\theta_i}^{\text{ref}} \sim N(0,1)$ for students in the reference group and ${\theta_i}^{\text{foc}} \sim N(-1,1)$ for students in the focal group. Finally, they generate item responses according to the Rasch model, which specifies that the probability of student $i$ responding correctly to item $j$ is
\begin{align}
\text{Pr}(y_{ij} = 1 | \theta_i, b_j) = \sigma(\theta_i + b_j) \label{zopf}
\end{align}
where $\sigma(x) = \frac{e^x}{e^x + 1}$ is the standard logistic function.

<!-- future improvement would be 1) show that for some student they equal just to make it ultra clear and 2) note that there are infinite possibilities the key is that the second term is -0.6 for students from the focal group -->

For every DIF simulation study framed in terms of item parameters that vary across groups, there is a mathematically equivalent setup in which students' abilities are multidimensional. The two-dimensional compensatory 2PL model [@thissen1986taxonomy] asserts that the probability that student $i$ responds correctly to item $j$ is
\begin{align}
\text{Pr}(y_{ij} = 1 | \theta_i, \eta_i, a_{\theta j}, a_{\eta j}, b_j) = \sigma(a_{\theta j}\theta_i + a_{\eta j}\eta_i + b_j).
\end{align}
where $\theta$ is target ability, $\eta$ is nuisance ability, $a_{\theta j}$ is an item's slope on target ability, and $a_{\eta j}$ is an item's slope on nuisance ability. 

We can use this model to translate the @kopf2015framework simulation from the DIF-as-item-property to the DIF-as-student-property view; $b_j$ is set to ${b_j}^{\text{ref}}$, $\theta$ stays the same, $a_{\theta j} = 1$ for all items (consistent with the Rasch model), and $\eta$ takes one of two values: ${\eta_i}^{\text{ref}} = 0$ for students in the reference group and ${\eta_i}^{\text{foc}} = -1$ for students in the focal group. For items with DIF, $a_{\eta j} = 0.6$, and for items without DIF, $a_{\eta j} = 0$. Again, 0.6 is the magnitude of DIF in logits. ^[The reader may notice that this is just one possible specification. Any specification that meets the following criteria has the same effect: First, $a_{\eta j}\eta_i$ must be 0.6 lower for the focal group on items with DIF. Second, $a_{\eta j}\eta_i$ must be equal across groups for items without DIF.] For a choice of student (from the focal or reference group) and item (with DIF or without), this model produces identical probabilities as Eqn \ref{zopf}. 

However, this translation between views reveals an important point. As a side effect of this approach, simulation studies have tended to focus on the unrealistic scenario in which there is no variation in the nuisance ability for students in the same group. Much as students vary in the target ability—a belief in such variation is central to the measurement enterprise—we suspect they also vary on additional ability dimensions. Indeed, previous work has shown that the best fitting model for item response data is often multidimensional where each dimension has significant variation [e.g., @furlow2009impact]. These dimensions are typically positively correlated, and the greater that correlation, the better unidimensional models tend to perform [@kirisci2001robustness]. Explicitly accounting for this variation—which we do in our simulation study—should yield results more indicative of how DIF methods will perform in practice.  

<!-- i might be a bit share on this here. it's basically do you have a q matrix you believe in. there must be some work with cross-validating different q matrices and then believing that. klint said he had some ideas here too. maybe things to think about in the future -->

If we insist on describing simulation conditions from the DIF-as-student-property view, one might wonder the following: Why not fit a multidimensional item response model which describes the data fully instead of looking for bias in a lower dimensional model? @camilli1992conceptual tested this idea with the goal of a "more satisfying description of the secondary abilities" [p. 144]. He found that the rotational indeterminacy of item response models is challenging to overcome and concluded that "a priori knowledge of the true factor structure" is necessary [p. 144]. It's hard to imagine how one would have such knowledge. Therefore, the best approach, which the DIF literature has nearly unanimously taken, is to fit unidimensional item response models and then look for bias manifesting in the item parameters. This approach depends upon a crucial identifying assumption, which we discuss at length below, is the target of our investigation.
